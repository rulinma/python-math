{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f4779e-0038-495d-93b3-8aa21525d4c8",
   "metadata": {},
   "source": [
    "# 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f59ec-4ddd-4ee6-88ae-b4c9ad73bc9e",
   "metadata": {},
   "source": [
    "f(x) = $x^{2}$ + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77583b9-19e0-4923-ac25-e113d545f65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 次迭代：x 值为  6.0\n",
      "第 1 次迭代：x 值为  3.5999999999999996\n",
      "第 2 次迭代：x 值为  2.1599999999999997\n",
      "第 3 次迭代：x 值为  1.2959999999999998\n",
      "第 4 次迭代：x 值为  0.7775999999999998\n",
      "第 5 次迭代：x 值为  0.46655999999999986\n",
      "第 6 次迭代：x 值为  0.2799359999999999\n",
      "第 7 次迭代：x 值为  0.16796159999999993\n",
      "第 8 次迭代：x 值为  0.10077695999999996\n",
      "第 9 次迭代：x 值为  0.06046617599999997\n",
      "第 10 次迭代：x 值为  0.036279705599999976\n",
      "第 11 次迭代：x 值为  0.021767823359999987\n",
      "第 12 次迭代：x 值为  0.013060694015999992\n",
      "第 13 次迭代：x 值为  0.007836416409599995\n",
      "第 14 次迭代：x 值为  0.004701849845759997\n",
      "第 15 次迭代：x 值为  0.002821109907455998\n",
      "第 16 次迭代：x 值为  0.0016926659444735988\n",
      "第 17 次迭代：x 值为  0.0010155995666841593\n",
      "第 18 次迭代：x 值为  0.0006093597400104956\n",
      "第 19 次迭代：x 值为  0.0003656158440062973\n",
      "第 20 次迭代：x 值为  0.0002193695064037784\n",
      "第 21 次迭代：x 值为  0.00013162170384226703\n",
      "第 22 次迭代：x 值为  7.897302230536021e-05\n",
      "第 23 次迭代：x 值为  4.7383813383216124e-05\n",
      "第 24 次迭代：x 值为  2.8430288029929674e-05\n",
      "第 25 次迭代：x 值为  1.7058172817957805e-05\n",
      "第 26 次迭代：x 值为  1.0234903690774682e-05\n",
      "第 27 次迭代：x 值为  6.1409422144648085e-06\n",
      "第 28 次迭代：x 值为  3.684565328678885e-06\n",
      "第 29 次迭代：x 值为  2.210739197207331e-06\n",
      "第 30 次迭代：x 值为  1.3264435183243986e-06\n",
      "第 31 次迭代：x 值为  7.958661109946391e-07\n",
      "第 32 次迭代：x 值为  4.775196665967835e-07\n",
      "局部最小值 x = 4.775196665967835e-07\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "一维问题的梯度下降法示例\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def func_1d(x):\n",
    "    \"\"\"\n",
    "    目标函数\n",
    "    :param x: 自变量，标量\n",
    "    :return: 因变量，标量\n",
    "    \"\"\"\n",
    "    return x ** 2 + 1\n",
    "\n",
    "\n",
    "def grad_1d(x):\n",
    "    \"\"\"\n",
    "    目标函数的梯度\n",
    "    :param x: 自变量，标量\n",
    "    :return: 因变量，标量\n",
    "    \"\"\"\n",
    "    return x * 2\n",
    "\n",
    "\n",
    "def gradient_descent_1d(grad, cur_x=0.1, learning_rate=0.01, precision=0.0001, max_iters=10000):\n",
    "    \"\"\"\n",
    "    一维问题的梯度下降法\n",
    "    :param grad: 目标函数的梯度\n",
    "    :param cur_x: 当前 x 值，通过参数可以提供初始值\n",
    "    :param learning_rate: 学习率，也相当于设置的步长\n",
    "    :param precision: 设置收敛精度\n",
    "    :param max_iters: 最大迭代次数\n",
    "    :return: 局部最小值 x*\n",
    "    \"\"\"\n",
    "    for i in range(max_iters):\n",
    "        grad_cur = grad(cur_x)\n",
    "        if abs(grad_cur) < precision:\n",
    "            break  # 当梯度趋近为 0 时，视为收敛\n",
    "        cur_x = cur_x - grad_cur * learning_rate\n",
    "        print(\"第\", i, \"次迭代：x 值为 \", cur_x)\n",
    "\n",
    "    print(\"局部最小值 x =\", cur_x)\n",
    "    return cur_x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gradient_descent_1d(grad_1d, cur_x=10, learning_rate=0.2, precision=0.000001, max_iters=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ba732-aea2-49e9-a802-28f24877a598",
   "metadata": {},
   "source": [
    "## f(x,y) = $-e^{-(x^2 + y^2）}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b64b6c5-1331-44f2-99b1-162d140cbf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1] 作为初始值开始迭代...\n",
      "第 0 次迭代：x 值为  [ 0.94586589 -0.94586589]\n",
      "第 1 次迭代：x 值为  [ 0.88265443 -0.88265443]\n",
      "第 2 次迭代：x 值为  [ 0.80832661 -0.80832661]\n",
      "第 3 次迭代：x 值为  [ 0.72080448 -0.72080448]\n",
      "第 4 次迭代：x 值为  [ 0.61880589 -0.61880589]\n",
      "第 5 次迭代：x 值为  [ 0.50372222 -0.50372222]\n",
      "第 6 次迭代：x 值为  [ 0.3824228 -0.3824228]\n",
      "第 7 次迭代：x 值为  [ 0.26824673 -0.26824673]\n",
      "第 8 次迭代：x 值为  [ 0.17532999 -0.17532999]\n",
      "第 9 次迭代：x 值为  [ 0.10937992 -0.10937992]\n",
      "第 10 次迭代：x 值为  [ 0.06666242 -0.06666242]\n",
      "第 11 次迭代：x 值为  [ 0.04023339 -0.04023339]\n",
      "第 12 次迭代：x 值为  [ 0.02419205 -0.02419205]\n",
      "第 13 次迭代：x 值为  [ 0.01452655 -0.01452655]\n",
      "第 14 次迭代：x 值为  [ 0.00871838 -0.00871838]\n",
      "第 15 次迭代：x 值为  [ 0.00523156 -0.00523156]\n",
      "第 16 次迭代：x 值为  [ 0.00313905 -0.00313905]\n",
      "第 17 次迭代：x 值为  [ 0.00188346 -0.00188346]\n",
      "第 18 次迭代：x 值为  [ 0.00113008 -0.00113008]\n",
      "第 19 次迭代：x 值为  [ 0.00067805 -0.00067805]\n",
      "第 20 次迭代：x 值为  [ 0.00040683 -0.00040683]\n",
      "第 21 次迭代：x 值为  [ 0.0002441 -0.0002441]\n",
      "第 22 次迭代：x 值为  [ 0.00014646 -0.00014646]\n",
      "第 23 次迭代：x 值为  [ 8.78751305e-05 -8.78751305e-05]\n",
      "第 24 次迭代：x 值为  [ 5.27250788e-05 -5.27250788e-05]\n",
      "第 25 次迭代：x 值为  [ 3.16350474e-05 -3.16350474e-05]\n",
      "第 26 次迭代：x 值为  [ 1.89810285e-05 -1.89810285e-05]\n",
      "第 27 次迭代：x 值为  [ 1.13886171e-05 -1.13886171e-05]\n",
      "第 28 次迭代：x 值为  [ 6.83317026e-06 -6.83317026e-06]\n",
      "第 29 次迭代：x 值为  [ 4.09990215e-06 -4.09990215e-06]\n",
      "第 30 次迭代：x 值为  [ 2.45994129e-06 -2.45994129e-06]\n",
      "第 31 次迭代：x 值为  [ 1.47596478e-06 -1.47596478e-06]\n",
      "第 32 次迭代：x 值为  [ 8.85578865e-07 -8.85578865e-07]\n",
      "第 33 次迭代：x 值为  [ 5.31347319e-07 -5.31347319e-07]\n",
      "第 34 次迭代：x 值为  [ 3.18808392e-07 -3.18808392e-07]\n",
      "局部最小值 x = [ 3.18808392e-07 -3.18808392e-07]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "二维问题的梯度下降法示例\n",
    "\"\"\"\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def func_2d(x):\n",
    "    \"\"\"\n",
    "    目标函数\n",
    "    :param x: 自变量，二维向量\n",
    "    :return: 因变量，标量\n",
    "    \"\"\"\n",
    "    return - math.exp(-(x[0] ** 2 + x[1] ** 2))\n",
    "\n",
    "\n",
    "def grad_2d(x):\n",
    "    \"\"\"\n",
    "    目标函数的梯度\n",
    "    :param x: 自变量，二维向量\n",
    "    :return: 因变量，二维向量\n",
    "    \"\"\"\n",
    "    deriv0 = 2 * x[0] * math.exp(-(x[0] ** 2 + x[1] ** 2))\n",
    "    deriv1 = 2 * x[1] * math.exp(-(x[0] ** 2 + x[1] ** 2))\n",
    "    return np.array([deriv0, deriv1])\n",
    "\n",
    "\n",
    "def gradient_descent_2d(grad, cur_x=np.array([0.1, 0.1]), learning_rate=0.01, precision=0.0001, max_iters=10000):\n",
    "    \"\"\"\n",
    "    二维问题的梯度下降法\n",
    "    :param grad: 目标函数的梯度\n",
    "    :param cur_x: 当前 x 值，通过参数可以提供初始值\n",
    "    :param learning_rate: 学习率，也相当于设置的步长\n",
    "    :param precision: 设置收敛精度\n",
    "    :param max_iters: 最大迭代次数\n",
    "    :return: 局部最小值 x*\n",
    "    \"\"\"\n",
    "    print(f\"{cur_x} 作为初始值开始迭代...\")\n",
    "    for i in range(max_iters):\n",
    "        grad_cur = grad(cur_x)\n",
    "        if np.linalg.norm(grad_cur, ord=2) < precision:\n",
    "            break  # 当梯度趋近为 0 时，视为收敛\n",
    "        cur_x = cur_x - grad_cur * learning_rate\n",
    "        print(\"第\", i, \"次迭代：x 值为 \", cur_x)\n",
    "\n",
    "    print(\"局部最小值 x =\", cur_x)\n",
    "    return cur_x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gradient_descent_2d(grad_2d, cur_x=np.array([1, -1]), learning_rate=0.2, precision=0.000001, max_iters=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91724e0c-b2e6-4a0b-bb6b-8fcc5ceaf85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 -3] 作为初始值开始迭代...\n",
      "局部最小值 x = [ 3 -3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3, -3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_2d(grad_2d, cur_x=np.array([3, -3]), learning_rate=0.2, precision=0.000001, max_iters=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a9fece-1572-435b-a222-e36833cb589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.13798785e-08 -9.13798785e-08]\n"
     ]
    }
   ],
   "source": [
    "print(grad_2d(np.array([3, -3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b99a6-174f-4f5a-82b5-b3d6bb2a5809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:math]",
   "language": "python",
   "name": "conda-env-math-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
